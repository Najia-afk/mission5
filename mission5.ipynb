{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce440d15",
   "metadata": {},
   "source": [
    "# Olist E-commerce Customer Segmentation Project\n",
    "\n",
    "## Context\n",
    "This project involves analyzing Olist's e-commerce data to develop customer segmentation and key performance indicators (KPIs) for a Customer Experience Dashboard. The goal is to extract actionable insights for marketing campaigns and customer behavior analysis.\n",
    "\n",
    "## Project Objectives\n",
    "1. **Implement SQL queries** to extract data for the Customer Experience Dashboard.\n",
    "2. **Develop customer segmentation** to support targeted marketing campaigns.\n",
    "3. **Analyze customer behavior patterns** to identify trends and opportunities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a11182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Plotly to properly render in HTML exports\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# Set the renderer for notebook display\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Enable figure exports with all necessary dependencies embedded\n",
    "#pio.write_html_config.include_plotlyjs = 'cdn'\n",
    "#pio.write_html_config.include_mathjax = 'cdn'\n",
    "#pio.write_html_config.full_html = True\n",
    "\n",
    "# Configure global theme for consistent appearance\n",
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5376f7",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aefb16",
   "metadata": {},
   "source": [
    "### 1.1 Database Connection Setup\n",
    "- **Objective**: Establish a connection to the SQLite database and verify the available tables.\n",
    "- **Steps**:\n",
    "  1. Initialize the database connection.\n",
    "  2. List all available tables in the dataset.\n",
    "  3. Preview the first few rows of each table to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac27900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.sqlite_connector import DatabaseConnection\n",
    "import os\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Initialize database connection\n",
    "db_path = os.path.join(cwd, 'dataset', 'olist.db')\n",
    "db = DatabaseConnection(db_path)\n",
    "\n",
    "# Get all table names\n",
    "tables = db.get_table_names()\n",
    "print(\"Available tables:\", tables)\n",
    "\n",
    "# Read specific table\n",
    "for table in tables:\n",
    "    orders_df = db.read_table(table)\n",
    "    display(orders_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efb206",
   "metadata": {},
   "source": [
    "### 1.2 Data Overview\n",
    "- **Objective**: Understand the contents of the database.\n",
    "- **Key Tables**:\n",
    "  - **Orders**: Contains order history and delivery details.\n",
    "  - **Customers**: Includes customer demographic and geographical data.\n",
    "  - **Products**: Provides product details and categories.\n",
    "  - **Reviews**: Contains customer satisfaction reviews.\n",
    "  - **Sellers**: Includes seller information and performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455761f",
   "metadata": {},
   "source": [
    "### 1.3 Query 1: Late Deliveries Analysis\n",
    "- **Objective**: Identify recent orders (less than 3 months old) that were delivered with a delay of at least 3 days, excluding canceled orders.\n",
    "- **Insights**:\n",
    "  - Helps assess delivery performance and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_late_deliveries = \"\"\"\n",
    "    WITH delivery_stats AS (\n",
    "        SELECT MAX(order_purchase_timestamp) as latest_estimate\n",
    "        FROM orders\n",
    "        WHERE order_status != 'canceled'\n",
    "    ),\n",
    "    delayed_deliveries AS (\n",
    "        SELECT o.*\n",
    "        FROM orders o, delivery_stats d\n",
    "        WHERE \n",
    "            o.order_estimated_delivery_date IS NOT NULL AND o.order_delivered_customer_date IS NOT NULL\n",
    "            AND o.order_status != 'canceled'\n",
    "            AND o.order_delivered_customer_date > date(o.order_estimated_delivery_date, '+3 days')\n",
    "            AND o.order_purchase_timestamp BETWEEN \n",
    "                date(d.latest_estimate, '-3 months') \n",
    "                AND d.latest_estimate\n",
    "    )\n",
    "    SELECT * FROM delayed_deliveries;\n",
    "    \"\"\"\n",
    "df_late_deliveries = db.execute_query(query_late_deliveries)\n",
    "display(df_late_deliveries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_last_6months_deliveries = \"\"\"\n",
    "    WITH delivery_stats AS (\n",
    "        SELECT MAX(order_purchase_timestamp) as latest_estimate\n",
    "        FROM orders\n",
    "        WHERE order_status != 'canceled'\n",
    "    ),\n",
    "    delayed_deliveries AS (\n",
    "        SELECT o.*\n",
    "        FROM orders o, delivery_stats d\n",
    "        WHERE \n",
    "            o.order_estimated_delivery_date IS NOT NULL AND o.order_delivered_customer_date IS NOT NULL\n",
    "            AND o.order_status != 'canceled'\n",
    "            AND o.order_purchase_timestamp BETWEEN \n",
    "                date(d.latest_estimate, '-6 months') \n",
    "                AND d.latest_estimate\n",
    "    )\n",
    "    SELECT * FROM delayed_deliveries;\n",
    "    \"\"\"\n",
    "df_deliveries_6months = db.execute_query(query_last_6months_deliveries)\n",
    "display(df_deliveries_6months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.visualize_late_deliveries import visualize_late_deliveries\n",
    "\n",
    "# Create visualization for late deliveries\n",
    "fig = visualize_late_deliveries(df_deliveries_6months)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe90b6f",
   "metadata": {},
   "source": [
    "### 1.4 Query 2: High Revenue Sellers\n",
    "- **Objective**: Identify sellers who generated over 100,000 BRL in revenue from delivered orders.\n",
    "- **Insights**:\n",
    "  - Highlights top-performing sellers for potential partnerships or incentives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_high_revenue = \"\"\"\n",
    "SELECT \n",
    "    s.seller_id,\n",
    "    CAST(SUM(oi.price + oi.freight_value) AS INTEGER) as total_revenue\n",
    "FROM sellers s\n",
    "    JOIN order_items oi ON s.seller_id = oi.seller_id\n",
    "    JOIN orders o ON oi.order_id = o.order_id WHERE o.order_status = 'delivered'\n",
    "GROUP BY s.seller_id\n",
    "HAVING total_revenue > 100000\n",
    "ORDER BY total_revenue DESC;\n",
    "\"\"\"\n",
    "df_high_revenue = db.execute_query(query_high_revenue)\n",
    "display(df_high_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fadd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.visualize_top_sellers import visualize_top_sellers\n",
    "\n",
    "# Create visualization for high revenue sellers\n",
    "fig = visualize_top_sellers(df_high_revenue)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ea17b",
   "metadata": {},
   "source": [
    "### 1.5 Query 3: Engaged New Sellers\n",
    "- **Objective**: Identify new sellers (active for less than 3 months) who have sold more than 30 products.\n",
    "- **Insights**:\n",
    "  - Helps track the onboarding success of new sellers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b84d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_new_engaged_sellers = \"\"\"\n",
    "WITH seller_stats AS (\n",
    "    SELECT \n",
    "        s.seller_id,\n",
    "        MIN(o.order_purchase_timestamp) as first_sale,\n",
    "        COUNT(DISTINCT oi.order_id) as total_orders,\n",
    "        COUNT(oi.product_id) as total_products\n",
    "    FROM sellers s\n",
    "        JOIN order_items oi ON s.seller_id = oi.seller_id\n",
    "        JOIN orders o ON oi.order_id = o.order_id\n",
    "    GROUP BY s.seller_id\n",
    "),\n",
    "latest_purchase_date AS (\n",
    "    SELECT MAX(order_purchase_timestamp) as latest_purchase\n",
    "    FROM orders\n",
    ")\n",
    "\n",
    "SELECT s.*\n",
    "FROM seller_stats s, latest_purchase_date l\n",
    "    WHERE s.total_products > 30\n",
    "    AND s.first_sale >= date(l.latest_purchase, '-3 months')\n",
    "ORDER BY s.total_products DESC;\n",
    "\"\"\"\n",
    "df_new_engaged_sellers = db.execute_query(query_new_engaged_sellers)\n",
    "display(df_new_engaged_sellers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea3943",
   "metadata": {},
   "source": [
    "### 1.6 Query 4: Worst Reviewed Postal Codes\n",
    "- **Objective**: Identify the top 5 postal codes with the worst average review scores (minimum 30 reviews) in the last 12 months.\n",
    "- **Insights**:\n",
    "  - Pinpoints geographical areas with potential service or product quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_worst_postcodes = \"\"\"\n",
    "\n",
    "WITH latest_purchase_date AS (\n",
    "    SELECT MAX(order_purchase_timestamp) as latest_purchase\n",
    "    FROM orders\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    c.customer_zip_code_prefix,\n",
    "    COUNT(r.review_id) as review_count,\n",
    "    ROUND(AVG(r.review_score), 2) as avg_score\n",
    "FROM customers c, latest_purchase_date l\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    JOIN order_reviews r ON o.order_id = r.order_id\n",
    "WHERE r.review_creation_date >= date(l.latest_purchase, '-12 months')\n",
    "GROUP BY c.customer_zip_code_prefix\n",
    "HAVING review_count >= 30\n",
    "ORDER BY avg_score ASC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "df_worst_postcodes= db.execute_query(query_worst_postcodes)\n",
    "display(df_worst_postcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a3c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all_postcodes = \"\"\"\n",
    "\n",
    "WITH latest_purchase_date AS (\n",
    "    SELECT MAX(order_purchase_timestamp) as latest_purchase\n",
    "    FROM orders\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    c.customer_zip_code_prefix,\n",
    "    COUNT(r.review_id) as review_count,\n",
    "    ROUND(AVG(r.review_score), 2) as avg_score\n",
    "FROM customers c, latest_purchase_date l\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    JOIN order_reviews r ON o.order_id = r.order_id\n",
    "WHERE r.review_creation_date >= date(l.latest_purchase, '-12 months')\n",
    "GROUP BY c.customer_zip_code_prefix\n",
    "HAVING review_count >= 1\n",
    "ORDER BY avg_score ASC;\n",
    "\n",
    "\"\"\"\n",
    "df_all_postcodes= db.execute_query(query_all_postcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02336563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.visualize_customer_review_bubble import create_brazil_postcode_map\n",
    "# Create and display the map\n",
    "fig = create_brazil_postcode_map(df_all_postcodes)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a40bf4",
   "metadata": {},
   "source": [
    "# 2. Data Extraction and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32819",
   "metadata": {},
   "source": [
    "### 2.1 RFM Features\n",
    "- **Objective**: Calculate Recency, Frequency, and Monetary (RFM) metrics for each customer.\n",
    "  - **Recency**: Time since the last purchase.\n",
    "  - **Frequency**: Number of orders placed.\n",
    "  - **Monetary**: Total spending by the customer.\n",
    "- **Insights**:\n",
    "  - RFM metrics are foundational for customer segmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rfm = \"\"\"\n",
    "WITH last_purchase_date AS (\n",
    "    SELECT MAX(order_purchase_timestamp) as max_date\n",
    "    FROM orders\n",
    "    WHERE order_status = 'delivered'\n",
    ")\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    MAX(o.order_purchase_timestamp) as order_purchase_timestamp,\n",
    "    -- Recency\n",
    "    CAST(JULIANDAY(l.max_date) - JULIANDAY(MAX(o.order_purchase_timestamp)) AS INTERGER) as recency_days,\n",
    "    -- Frequency\n",
    "    COUNT(o.order_id) as frequency,\n",
    "    -- Monetary\n",
    "    CAST(SUM(oi.price + oi.freight_value) AS INTEGER) as monetary\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "CROSS JOIN last_purchase_date l\n",
    "WHERE o.order_status = 'delivered'\n",
    "GROUP BY c.customer_id;\n",
    "\"\"\"\n",
    "df_rfm= db.execute_query(query_rfm)\n",
    "display(df_rfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0378c265",
   "metadata": {},
   "source": [
    "### 2.2 Customer Satisfaction Metrics\n",
    "- **Objective**: Analyze customer satisfaction based on review data.\n",
    "  - **Average Review Score**: Overall satisfaction level.\n",
    "  - **Review Count**: Number of reviews submitted.\n",
    "  - **Negative Reviews**: Count of reviews with scores ≤ 2.\n",
    "- **Insights**:\n",
    "  - Helps identify dissatisfied customers and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_satisfaction = \"\"\"\n",
    "WITH customers_with_negative AS (\n",
    "    SELECT DISTINCT o.customer_id, 1 as has_negative_flag\n",
    "    FROM orders o\n",
    "    JOIN order_reviews r ON o.order_id = r.order_id\n",
    "    WHERE r.review_score <= 2\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    o.customer_id,\n",
    "    CAST(AVG(r.review_score) AS INTEGER) as avg_review_score,\n",
    "    COUNT(r.review_id) as review_count,\n",
    "    COALESCE(cwn.has_negative_flag, 0) as has_negative_flag\n",
    "FROM orders o\n",
    "JOIN order_reviews r ON o.order_id = r.order_id\n",
    "LEFT JOIN customers_with_negative cwn ON o.customer_id = cwn.customer_id\n",
    "GROUP BY o.customer_id;\n",
    "\"\"\"\n",
    "df_satisfaction = db.execute_query(query_satisfaction)\n",
    "display(df_satisfaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a502baa",
   "metadata": {},
   "source": [
    "### 2.3 Customer Behavior Patterns\n",
    "- **Objective**: Extract behavioral insights from customer purchase data.\n",
    "  - **Product Category Diversity**: Number of unique product categories purchased.\n",
    "  - **Average Delivery Time**: Time taken for orders to be delivered.\n",
    "  - **Unique Sellers**: Number of sellers a customer has interacted with.\n",
    "- **Insights**:\n",
    "  - Provides a deeper understanding of customer preferences and habits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_behavior = \"\"\"\n",
    "SELECT \n",
    "    o.customer_id,\n",
    "    COUNT( p.product_category_name) as unique_categories,\n",
    "    AVG(CAST(JULIANDAY(order_delivered_customer_date) - \n",
    "        JULIANDAY(order_purchase_timestamp) AS INTEGER)) as avg_delivery_time,\n",
    "    COUNT( oi.seller_id) as unique_sellers\n",
    "FROM orders o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE o.order_status = 'delivered'\n",
    "GROUP BY o.customer_id;\n",
    "\"\"\"\n",
    "df_behavior= db.execute_query(query_behavior)\n",
    "display(df_behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4301b81",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa67f64",
   "metadata": {},
   "source": [
    "### 3.1 Merge Features\n",
    "- **Objective**: Combine RFM metrics, satisfaction metrics, and behavior patterns into a unified dataset for analysis.\n",
    "- **Outcome**:\n",
    "  - A consolidated DataFrame ready for further analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb57b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.feature_analysis import FeatureAnalysis\n",
    "from src.classes.feature_engineering import FeatureEngineering\n",
    "\n",
    "# Initialize with existing query results\n",
    "fe = FeatureEngineering(df_rfm, df_satisfaction, df_behavior)\n",
    "features_df = fe.combine_features()\n",
    "\n",
    "\n",
    "\n",
    "# Plot results\n",
    "fa = FeatureAnalysis(features_df, columns_to_exclude=['order_id', 'customer_id'])\n",
    "dist_plot = fa.plot_distributions()\n",
    "dist_plot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a35c8",
   "metadata": {},
   "source": [
    "### 3.2 Feature Distribution Analysis\n",
    "- **Objective**: Explore the distribution of features to identify patterns, outliers, and preprocessing needs.\n",
    "- **Steps**:\n",
    "  1. **Statistical Summaries**: Compute mean, median, standard deviation, etc.\n",
    "  2. **Distribution Plots**: Visualize feature distributions using histograms or density plots.\n",
    "  3. **Outlier Detection**: Use box plots to identify extreme values.\n",
    "- **Outcome**:\n",
    "  - Informs decisions on scaling, normalization, and handling outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4285dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.analyze_numerical_outliers import analyze_outliers_with_multiple_methods\n",
    "\n",
    "# Analyze all numeric variables with different outlier detection methods\n",
    "all_summaries, all_cleaned_dfs = analyze_outliers_with_multiple_methods(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70990bfc",
   "metadata": {},
   "source": [
    "### 3.3 Box Plot Analysis\n",
    "- **Objective**: Visualize feature distributions and detect outliers.\n",
    "- **Steps**:\n",
    "  1. Plot box plots for numerical features.\n",
    "  2. Highlight features with significant outliers.\n",
    "- **Outcome**:\n",
    "  - Guides feature scaling and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6700a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.visualize_numerical_outliers import create_interactive_outlier_visualization\n",
    "\n",
    "# Create the interactive outlier visualization\n",
    "summary_df, df_cleaned = create_interactive_outlier_visualization(all_cleaned_dfs['Z-score (±3)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d18b4",
   "metadata": {},
   "source": [
    "### 3.4 RFM dashboard review after normalization\n",
    "- **Objective**: Visualize RFM feature distributions after removal of outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hybrid dataframe with cleaned continuous variables but original flags/counts\n",
    "hybrid_df = all_cleaned_dfs['Z-score (±3)'].copy()\n",
    "\n",
    "# List of columns to restore from original data\n",
    "categorical_cols = ['has_negative_flag'] \n",
    "count_cols = ['review_count', 'unique_categories', 'frequency', 'unique_sellers']\n",
    "\n",
    "# Replace the values with originals\n",
    "for col in categorical_cols + count_cols:\n",
    "    if col in features_df.columns and col in hybrid_df.columns:\n",
    "        hybrid_df[col] = features_df[col]\n",
    "\n",
    "# Use the hybrid dataframe for visualization\n",
    "fa = FeatureAnalysis(hybrid_df, columns_to_exclude=['customer_id'])\n",
    "dist_plot = fa.plot_distributions()\n",
    "dist_plot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ac67c",
   "metadata": {},
   "source": [
    "### 3.3 Feature Correlations\n",
    "- **Objective**: Analyze relationships between features using a correlation matrix.\n",
    "- **Steps**:\n",
    "  1. Compute the correlation matrix for numerical features.\n",
    "  2. Visualize the lower triangle of the matrix using a heatmap.\n",
    "- **Outcome**:\n",
    "  - Identifies highly correlated features and potential redundancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da648a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "corr_plot = fa.plot_correlation_matrix()\n",
    "corr_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd402050",
   "metadata": {},
   "source": [
    "## 4. Implement clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fe653",
   "metadata": {},
   "source": [
    "# 4.1 Feature Transformations for Clustering\n",
    "\n",
    "The feature transformation process implements a sophisticated approach to prepare customer data for clustering, ensuring optimal algorithm performance while maintaining interpretability:\n",
    "\n",
    "## Transformation Components\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Set customer_id as the index to maintain customer identity throughout the analysis\n",
    "   - Removed temporal variables (order_purchase_timestamp) that aren't relevant for segmentation\n",
    "   - Created a clean foundation for applying consistent transformations\n",
    "\n",
    "2. **Specialized Transformation Strategy**\n",
    "   - Implemented the `GenericFeatureTransformer` to handle different variable types appropriately\n",
    "   - Applied specific transformation methods to three distinct variable categories:\n",
    "     * **Numerical variables**: Applied robust scaling to minimize outlier impact\n",
    "     * **Categorical indicators**: Properly encoded the has_negative_flag variable\n",
    "     * **Count variables**: Applied special handling to preserve the count nature while standardizing scale\n",
    "\n",
    "3. **Count Variable Preservation**\n",
    "   - Recognized that count variables (review_count, unique_categories, frequency, unique_sellers) have special properties\n",
    "   - Implemented specialized transformations that maintain their inherent characteristics\n",
    "   - Ensured that zero counts remain meaningful in the transformed space\n",
    "\n",
    "4. **Reversible Transformations**\n",
    "   - Stored transformation parameters to enable inverse transformation\n",
    "   - Created capability to convert clusters back to original feature space\n",
    "   - Ensured business stakeholders can understand results in familiar metrics\n",
    "\n",
    "## Outcome\n",
    "\n",
    "The transformation process creates a dataset that:\n",
    "\n",
    "- Places all variables on comparable scales for distance-based clustering algorithms\n",
    "- Preserves the inherent structure and relationships between variables\n",
    "- Minimizes the impact of outliers without losing their information\n",
    "- Maintains the ability to interpret results in the original business context\n",
    "\n",
    "This approach balances mathematical optimization with business interpretability, enabling both effective clustering and actionable insights from the resulting segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "895b9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.feature_transformation import GenericFeatureTransformer\n",
    "\n",
    "#Set customer_id as index before transformation\n",
    "hybrid_df = hybrid_df.set_index('customer_id')\n",
    "hybrid_df = hybrid_df.drop(columns=['order_purchase_timestamp'], errors='ignore')\n",
    "\n",
    "# Create and apply the generic transformer\n",
    "transformer = GenericFeatureTransformer()\n",
    "transformed_df = transformer.fit_transform(\n",
    "    df=hybrid_df,\n",
    "    categorical_cols=['has_negative_flag'],\n",
    "    count_cols=['review_count', 'unique_categories', 'frequency', 'unique_sellers']\n",
    ")\n",
    "\n",
    "# Get original data back when needed\n",
    "original_df = transformer.inverse_transform(transformed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6612549",
   "metadata": {},
   "source": [
    "# 4.2 Clustering Analysis with Advanced Feature Importance\n",
    "\n",
    "The customer segmentation implementation uses a sophisticated dual-model approach combining PCA transformation with robust feature importance analysis:\n",
    "\n",
    "## Analysis Components\n",
    "\n",
    "1. **PCA Component Analysis**\n",
    "   - Applied dimensionality reduction to visualize high-dimensional customer data\n",
    "   - Created explained variance plots to identify optimal components (87% variance captured in top 3 components)\n",
    "   - Generated biplots showing feature loadings on principal components for interpretability\n",
    "   - Identified which features contribute most to each principal component\n",
    "\n",
    "2. **Optimal Cluster Determination**\n",
    "   - Implemented optimized elbow method with silhouette scoring\n",
    "   - Analyzed inertia and silhouette metrics to identify the ideal number of clusters\n",
    "   - Selected 3 clusters based on the balance between complexity and interpretability\n",
    "\n",
    "3. **Dual-Model K-Means Implementation**\n",
    "   - Trained PCA-based model for visualization (using 3D PCA components)\n",
    "   - Trained feature-space model for interpretability (using original features)\n",
    "   - Maintained both models to maximize insights while enabling visualization\n",
    "\n",
    "4. **Feature Importance Analysis**\n",
    "   - Applied permutation importance to identify cluster-defining features\n",
    "   - Used silhouette score reduction as the importance metric\n",
    "   - Visualized with error bars to show consistency of importance scores\n",
    "   - Sampling techniques applied for computational efficiency\n",
    "\n",
    "## Outcome\n",
    "\n",
    "The analysis reveals distinct customer segments with clear behavioral differences, enabling targeted marketing strategies based on:\n",
    "\n",
    "- Recency, frequency, and monetary (RFM) profile of each segment\n",
    "- Satisfaction levels and complaint patterns\n",
    "- Product category diversity and seller engagement patterns\n",
    "\n",
    "The implementation balances computation speed and insight depth, providing actionable customer segmentation with clear understanding of which features define each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100daa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.classes.cluster_analysis\n",
    "importlib.reload(src.classes.cluster_analysis)\n",
    "from src.classes.cluster_analysis import ClusteringAnalysis\n",
    "\n",
    "# Initialize clustering with the transformer for inverse transform capability\n",
    "ca = ClusteringAnalysis(\n",
    "    df=transformed_df, \n",
    "    transformer=transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06384ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate elbow plot to determine optimal number of clusters\n",
    "elbow_plot = ca.plot_elbow(range(2, 11))\n",
    "elbow_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01bb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA explained variance to select optimal components\n",
    "fig_pca_variance = ca.plot_pca_explained_variance(max_components=15)\n",
    "fig_pca_variance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA biplot to visualize feature loadings on principal components\n",
    "fig_biplot = ca.plot_pca_biplot(n_features=5)\n",
    "fig_biplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdde09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance in principal components\n",
    "fig_pca_importance = ca.plot_pca_feature_importance(n_components=4)\n",
    "fig_pca_importance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustering with a different number of clusters (e.g., 5)\n",
    "ca.fit_kmeans(n_clusters=4)\n",
    "\n",
    "# Create and show silhouette visualization to evaluate cluster quality\n",
    "silhouette_fig = ca.plot_silhouette_plotly()\n",
    "silhouette_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display circle-based intercluster visualization\n",
    "intercluster_fig = ca.plot_intercluster_distance_circles()\n",
    "intercluster_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display circle-based intercluster visualization\n",
    "kmeansfeature_fig = ca.plot_kmeans_feature_importance()\n",
    "kmeansfeature_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd85daf",
   "metadata": {},
   "source": [
    "## Understanding the Clustering Approach\n",
    "\n",
    "The clustering implementation uses a sophisticated dual-model approach:\n",
    "\n",
    "1. **PCA-based Clustering Model**:\n",
    "   - Used for the elbow method and visualizations\n",
    "   - Applied to dimensionally-reduced data (PCA components)\n",
    "   - Advantages: Better visualization, handles correlated features\n",
    "\n",
    "2. **Original Feature Space Clustering Model**:\n",
    "   - Used for feature importance analysis\n",
    "   - Applied directly to transformed features (not PCA)\n",
    "   - Advantages: Direct interpretability of feature effects\n",
    "\n",
    "This dual approach explains why:\n",
    "- The elbow plot may suggest different optimal clusters than silhouette analysis\n",
    "- Feature importance relates to the original features, not PCA components\n",
    "- We can both visualize clusters effectively and understand feature contributions## Understanding the Clustering Approach\n",
    "\n",
    "The clustering implementation uses a sophisticated dual-model approach:\n",
    "\n",
    "1. **PCA-based Clustering Model**:\n",
    "   - Used for the elbow method and visualizations\n",
    "   - Applied to dimensionally-reduced data (PCA components)\n",
    "   - Advantages: Better visualization, handles correlated features\n",
    "\n",
    "2. **Original Feature Space Clustering Model**:\n",
    "   - Used for feature importance analysis\n",
    "   - Applied directly to transformed features (not PCA)\n",
    "   - Advantages: Direct interpretability of feature effects\n",
    "\n",
    "This dual approach explains why:\n",
    "- The elbow plot may suggest different optimal clusters than silhouette analysis\n",
    "- Feature importance relates to the original features, not PCA components\n",
    "- We can both visualize clusters effectively and understand feature contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom segment configuration - adjust thresholds based on business requirements\n",
    "custom_segment_config = {\n",
    "    # Recency thresholds (in days)\n",
    "    'recency': {\n",
    "        'active': 45,      # 0-45 days: \"Active\" (adjusted from default 30)\n",
    "        'recent': 120,     # 46-120 days: \"Recent\" (adjusted from default 90)\n",
    "        # > 120 days: \"Inactive\"\n",
    "    },\n",
    "    \n",
    "    # Frequency thresholds (number of orders)\n",
    "    'frequency': {\n",
    "        'frequent': 2.5,   # > 2.5 orders: \"Frequent\" (adjusted from default 3)\n",
    "        'returning': 1.2,  # > 1.2 orders: \"Returning\" (adjusted from default 1.5)\n",
    "        # <= 1.2 orders: \"One-time\"\n",
    "    },\n",
    "    \n",
    "    # Monetary thresholds\n",
    "    'monetary': {\n",
    "        'high_value': 150,  # > $150: \"High-value\" (fixed value instead of default 'mean')\n",
    "        # <= $150: \"Standard-value\" \n",
    "    },\n",
    "    \n",
    "    # Satisfaction thresholds (review scores)\n",
    "    'satisfaction': {\n",
    "        'very_satisfied': 4.7,  # >= 4.7: \"Very Satisfied\" (adjusted from default 4.5)\n",
    "        'satisfied': 4.2,       # >= 4.2: \"Satisfied\" (adjusted from default 4.0)\n",
    "        'neutral': 3.5,         # >= 3.5: \"Neutral\" (adjusted from default 3.0)\n",
    "        # < 3.5: \"Unsatisfied\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "from src.scripts.cluster_dashboard import create_sales_dashboard\n",
    "import numpy as np\n",
    "\n",
    "# Store the original dataframe in the transformer for reference\n",
    "transformer.original_df = hybrid_df\n",
    "\n",
    "# Get the cluster labels from the clustering model\n",
    "labels = ca.kmeans_results['labels']\n",
    "print(f\"Found {len(labels)} labels across {len(np.unique(labels))} clusters\")\n",
    "\n",
    "# Create the dashboard with custom segment configuration\n",
    "dashboard = create_sales_dashboard(labels, transformed_df, transformer, custom_segment_config)\n",
    "\n",
    "# Display the components\n",
    "for name, fig in dashboard.items():\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64b1fe",
   "metadata": {},
   "source": [
    "## 5. Evaluate Segment Stability\n",
    "- **Objective**: Assess the stability of customer segments over time.\n",
    "- **Steps**:\n",
    "  1. Validate feature consistency across time periods.\n",
    "  2. Compute stability scores for each segment.\n",
    "  3. Visualize stability trends.\n",
    "- **Outcome**:\n",
    "  - Ensures segments remain meaningful and actionable over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.stability_analysis import ClusterStabilityAnalysis\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the original dataframe with dates for temporal analysis\n",
    "# We need to merge the order_purchase_timestamp back in\n",
    "orders_df = db.read_table('orders')\n",
    "orders_df['order_purchase_timestamp'] = pd.to_datetime(orders_df['order_purchase_timestamp'])\n",
    "\n",
    "# Extract just the customer_id and timestamp columns\n",
    "orders_date_df = orders_df[['customer_id', 'order_purchase_timestamp']]\n",
    "\n",
    "# Group by customer_id and get the most recent order date\n",
    "customer_dates = orders_date_df.groupby('customer_id').agg(\n",
    "    order_purchase_timestamp=('order_purchase_timestamp', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Merge with the original dataframe (use the index which should be customer_id)\n",
    "original_df_with_dates = hybrid_df.copy()\n",
    "original_df_with_dates = original_df_with_dates.reset_index()\n",
    "original_df_with_dates = pd.merge(\n",
    "    original_df_with_dates,\n",
    "    customer_dates,\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create a mapping between positions and original customer IDs\n",
    "original_customer_ids = original_df_with_dates['customer_id'].values\n",
    "index_mapping = {i: customer_id for i, customer_id in enumerate(original_customer_ids)}\n",
    "\n",
    "# Reindex transformed_df with original customer IDs\n",
    "transformed_df_with_ids = transformed_df.copy()\n",
    "transformed_df_with_ids.index = pd.Index([index_mapping.get(i, i) for i in transformed_df.index])\n",
    "\n",
    "\n",
    "# Initialize the stability analysis\n",
    "stability_analyzer = ClusterStabilityAnalysis(\n",
    "    df=transformed_df_with_ids,\n",
    "    transformer=transformer,\n",
    "    original_df_with_dates=original_df_with_dates\n",
    ")\n",
    "\n",
    "# Evaluate bootstrap stability (random sampling approach)\n",
    "bootstrap_results = stability_analyzer.evaluate_bootstrap_stability(\n",
    "    n_clusters=4,  # Same number used in your clustering\n",
    "    n_iterations=20,  # Number of bootstrap samples\n",
    "    sample_fraction=0.8  # Sample 80% of data each time\n",
    ")\n",
    "\n",
    "# Display bootstrap stability results\n",
    "bootstrap_results['figure'].show()\n",
    "\n",
    "\n",
    "# Evaluate cross-period stability (works even without customer overlap)\n",
    "cross_period_results = stability_analyzer.evaluate_cross_period_stability(\n",
    "    n_clusters=4,\n",
    "    period='quarter',\n",
    "    eval_sample_size=1000,\n",
    "    min_customers_per_period=10  # Lower threshold for testing\n",
    ")\n",
    "\n",
    "# Display results\n",
    "cross_period_results['figure'].show()\n",
    "cross_period_results['figure_counts'].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e97ac",
   "metadata": {},
   "source": [
    "## 6. Maintenance Recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mission5_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
